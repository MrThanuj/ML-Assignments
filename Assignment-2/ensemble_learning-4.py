# -*- coding: utf-8 -*-
"""ensemble_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_fRks5B_Of4m35scnD7Wl35cVMz22tiB
"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_iris


def ensemble_learning(X, y, task='classification'):
    """
    Implement ensemble learning methods: Decision Trees, Bagging, Random Forest, and Boosting.

    Parameters:
    - X: Input features (numpy array or pandas DataFrame)
    - y: Target variable (numpy array or pandas Series)
    - task: Type of task, either 'classification' or 'regression' (default: 'classification')

    Returns:
    - results: Dictionary containing evaluation results for each ensemble method
    """
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Initialize results dictionary
    results = {}

    # Decision Tree
    dt_classifier = DecisionTreeClassifier(max_depth=3, min_samples_split=8)
    # Train the Decision Tree classifier on the training data
    dt_classifier.fit(X_train, y_train)

    # Predict the labels for the testing data
    y_pred_dt = dt_classifier.predict(X_test)

    # Calculate the accuracy of the Decision Tree classifier
    accuracy_dt = accuracy_score(y_test, y_pred_dt)

    # Store the accuracy in the results dictionary
    results['Decision Tree'] = accuracy_dt

    # Bagging
    bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=2, random_state=42)
    # Train the Bagging classifier on the training data
    bagging_classifier.fit(X_train, y_train)

    # Predict the labels for the testing data
    y_pred_bagging = bagging_classifier.predict(X_test)

    # Calculate the accuracy of the Bagging classifier
    accuracy_bagging = accuracy_score(y_test, y_pred_bagging)

    # Store the accuracy in the results dictionary
    results['Bagging'] = accuracy_bagging

    # Random Forest
    rf_classifier = RandomForestClassifier(n_estimators=2, random_state=42)
    # Train the Random Forest classifier on the training data
    rf_classifier.fit(X_train, y_train)

    # Predict the labels for the testing data
    y_pred_rf = rf_classifier.predict(X_test)

    # Calculate the accuracy of the Random Forest classifier
    accuracy_rf = accuracy_score(y_test, y_pred_rf)

    # Store the accuracy in the results dictionary
    results['Random Forest'] = accuracy_rf

    # Boosting
    if task == 'classification':
        boosting_classifier = AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=2, random_state=42)
        boosting_classifier.fit(X_train, y_train)
        y_pred_boosting = boosting_classifier.predict(X_test)
        accuracy_boosting = accuracy_score(y_test, y_pred_boosting)
        results['Boosting'] = accuracy_boosting
    else:
        # For regression tasks, you can use GradientBoostingRegressor instead
        #pass
    # Train the Boosting classifier on the training data
        boosting_regressor = GradientBoostingRegressor(n_estimators=2, random_state=42)
        boosting_regressor.fit(X_train, y_train)

    # Predict the labels for the testing data
        y_pred_boosting = boosting_regressor.predict(X_test)

    # Calculate the accuracy of the Boosting classifier
        mse_boosting = mean_squared_error(y_test, y_pred_boosting)

    # Store the accuracy in the results dictionary
        results['Boosting'] = mse_boosting

    return results

# Example usage:
iris = load_iris()
X = iris.data
y = iris.target
results = ensemble_learning(X, y, task='classification')
print(results)